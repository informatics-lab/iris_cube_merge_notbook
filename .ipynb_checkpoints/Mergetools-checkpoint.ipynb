{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install boto\n",
    "!pip install git+git://github.com/met-office-lab/asn_data_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import iris\n",
    "from asn_utils.Loader import Loader\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import distributed\n",
    "import dask\n",
    "from dask import bag as db\n",
    "\n",
    "import itertools\n",
    "\n",
    "client = distributed.Client('dask.informaticslab.co.uk:8786')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 files for example : /usr/local/share/notebooks/data/mogreps/201612/prods_op_mogreps-g_20161203_00_00_003.pp\n"
     ]
    }
   ],
   "source": [
    "myloader = Loader()\n",
    "fs = myloader.list_files(\"mogreps\")\n",
    "fs = fs[:20]\n",
    "print ('%s files for example : %s'%(len(fs), fs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Error(object):\n",
    "    def __init__(self, msg, err):\n",
    "        self.msg = msg\n",
    "        self.err = err\n",
    "    def __str__(self):\n",
    "        return ('Error(%s, %s)' % ( self.msg, self.err))\n",
    "\n",
    "def load_cubes(filepath, constraint):\n",
    "    if not os.path.exists(filepath):\n",
    "        filepath = '/data' + filepath.split('/data')[1]\n",
    "    try:\n",
    "        return iris.load_raw(filepath, constraint, callback=add_realisation)\n",
    "    except Exception as e:\n",
    "        return Error(str(e), e)\n",
    "    \n",
    "def realization_from_filename(filename):\n",
    "    return int(os.path.basename(filename).split('_')[-2])\n",
    "\n",
    "def add_realisation(cube, field, filename):\n",
    "    # have we got a realization attribute?\n",
    "    try:\n",
    "        realization_coord = cube.coord('realization')\n",
    "    except iris.exceptions.CoordinateNotFoundError:\n",
    "        realization = realization_from_filename(filename)\n",
    "        cube.add_aux_coord(iris.coords.AuxCoord(realization, standard_name='realization', units='1'))\n",
    "        \n",
    "def not_an_error(thing):\n",
    "    return not isinstance(thing, Error)\n",
    "\n",
    "        \n",
    "@dask.delayed\n",
    "def delayed_load(filename, constraint):\n",
    "    return load_cubes(filename, constraint)\n",
    "\n",
    "\n",
    "\n",
    "load_bag = db.from_delayed([delayed_load(f, 'relative_humidity') for f in fs])\n",
    "cubes = iris.cube.CubeList(load_bag.filter(not_an_error).compute())\n",
    "cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative_humidity / (%)             (latitude: 600; longitude: 800)\n",
      "     Dimension coordinates:\n",
      "          latitude                           x               -\n",
      "          longitude                          -               x\n",
      "     Scalar coordinates:\n",
      "          forecast_period: 0.0 hours\n",
      "          forecast_reference_time: 2016-12-03 00:00:00\n",
      "          pressure: 925.0 hPa\n",
      "          realization: 0\n",
      "          time: 2016-12-03 00:00:00\n",
      "     Attributes:\n",
      "          STASH: m01s16i256\n",
      "          source: Data from Met Office Unified Model\n",
      "          um_version: 10.4\n",
      "relative_humidity / (%)             (latitude: 600; longitude: 800)\n",
      "     Dimension coordinates:\n",
      "          latitude                           x               -\n",
      "          longitude                          -               x\n",
      "     Scalar coordinates:\n",
      "          forecast_period: 0.0 hours\n",
      "          forecast_reference_time: 2016-12-03 00:00:00\n",
      "          pressure: 1000.0 hPa\n",
      "          realization: 0\n",
      "          time: 2016-12-03 00:00:00\n",
      "     Attributes:\n",
      "          STASH: m01s16i256\n",
      "          source: Data from Met Office Unified Model\n",
      "          um_version: 10.4\n",
      "{'time': {'list': [411312.0, 411372.0], 'set': {411312.0, 411372.0}}, 'pressure': {'list': [200.0, 1000.0, 700.0], 'set': {200.0, 1000.0, 700.0}}, 'realization': {'list': [0], 'set': {0}}}\n",
      "(411372.0, 0, 200.0)\n",
      "[[[False False False]]\n",
      "\n",
      " [[ True False False]]]\n",
      "[(411312.0, 0, 200.0), (411312.0, 0, 1000.0), (411312.0, 0, 700.0), (411372.0, 0, 1000.0), (411372.0, 0, 700.0)]\n"
     ]
    },
    {
     "ename": "DuplicateDataError",
     "evalue": "failed to merge into a single cube.\n  Duplicate 'relative_humidity' cube, with scalar coordinates time=Cell(point=411312.0, bound=None), forecast_reference_time=Cell(point=411312.0, bound=None), forecast_period=Cell(point=0.0, bound=None), pressure=Cell(point=1000.0, bound=None), realization=Cell(point=0, bound=None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateDataError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-76a790a4fa0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m# print(subd[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# print(masked_merge(subd, ['time','realization', 'pressure']))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'realization'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pressure'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-76a790a4fa0e>\u001b[0m in \u001b[0;36mmasked_merge\u001b[0;34m(cube_list, ortha_coords)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mnew_cubes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_missing_cubes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_merged_cube\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mortha_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcube_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mall_cubes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcube_list\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnew_cubes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_cubes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_cube\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/iris/cube.py\u001b[0m in \u001b[0;36mmerge_cube\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;31m# Extract the merged cube from the ProtoCube.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mmerged_cube\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto_cube\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmerged_cube\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/iris/_merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, unique)\u001b[0m\n\u001b[1;32m   1204\u001b[0m         \u001b[0;31m# Check for unique data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munique\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgroup_depth\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_duplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnd_indexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_by_nd_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0;31m# Generate group-depth merged cubes from the source-cubes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/iris/_merge.py\u001b[0m in \u001b[0;36m_report_duplicate\u001b[0;34m(self, nd_indexes, group_by_nd_index)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Duplicate %r cube, with scalar coordinates %s'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDuplicateDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDuplicateDataError\u001b[0m: failed to merge into a single cube.\n  Duplicate 'relative_humidity' cube, with scalar coordinates time=Cell(point=411312.0, bound=None), forecast_reference_time=Cell(point=411312.0, bound=None), forecast_period=Cell(point=0.0, bound=None), pressure=Cell(point=1000.0, bound=None), realization=Cell(point=0, bound=None)"
     ]
    }
   ],
   "source": [
    "def make_masked_cube_for_point(template_cube, coord_names, point):\n",
    "        c = template_cube.copy()\n",
    "        constraints = {}\n",
    "        for name in coord_names:\n",
    "            constraints[name] = c.coord(name).points[0]\n",
    "        constraint = iris.Constraint(**constraints)\n",
    "        new_cube = c.extract(constraint)\n",
    "        for i, coord_name in enumerate(coord_names):\n",
    "            new_cube.coord(coord_name).points = (point[i],)\n",
    "        lazy_data = new_cube.lazy_data()\n",
    "        data = np.ma.masked_all(lazy_data.shape, dtype=lazy_data.dtype)\n",
    "        np.ma.set_fill_value(data, lazy_data.fill_value)\n",
    "        new_cube.data = data\n",
    "        return new_cube\n",
    "    \n",
    "def get_missing_points(cube_with_missing, dims):\n",
    "    refs = {}\n",
    "    shape = []\n",
    "    for dim in dims:\n",
    "        a_set = set(cube_with_missing.coord(dim).points)\n",
    "        a_list = list(a_set)\n",
    "        refs[dim] = {'set':a_set, 'list':a_list}\n",
    "        shape.append(len(a_list))\n",
    "        \n",
    "    print(refs)\n",
    "    problem_space = np.zeros(shape, dtype=bool)\n",
    "    \n",
    "    for point_in_cube_space in zip(*(cube_with_missing.coord(dim).points for dim in dims)):\n",
    "        print(point_in_cube_space)\n",
    "        point_in_problem_space = tuple((refs[dims[i]]['list'].index(point) for i, point in enumerate(point_in_cube_space)))\n",
    "\n",
    "        problem_space[point_in_problem_space] = True\n",
    "        \n",
    "    \n",
    "    missing = []\n",
    "    print (problem_space)\n",
    "    for missing_point_in_problem_space in zip(*np.where(problem_space == False)):\n",
    "        \n",
    "        point_in_cube_space = tuple((refs[dims[i]]['list'][point] for i, point in enumerate(missing_point_in_problem_space)))\n",
    "        missing.append(point_in_cube_space)\n",
    "    \n",
    "    return missing\n",
    "\n",
    "def make_missing_cubes(cube, coord_names, template_cube):\n",
    "    missing = get_missing_points(cube, coord_names)\n",
    "    print(missing)\n",
    "    cubes = []\n",
    "    for miss in missing:\n",
    "        cubes.append(make_masked_cube_for_point(template_cube, coord_names, miss))\n",
    "    return cubes\n",
    "\n",
    "\n",
    "def masked_merge(cube_list, ortha_coords):\n",
    "    simple_merged_cube = cube_list.merge_cube()\n",
    "    new_cubes = make_missing_cubes(simple_merged_cube, ortha_coords, cube_list[0])\n",
    "    all_cubes = cube_list + new_cubes\n",
    "    return all_cubes.merge_cube()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "subd = iris.cube.CubeList([cubes[0]] + [cubes[-6]]   + [cubes[-1]])\n",
    "\n",
    "\n",
    "\n",
    "merged = iris.cube.CubeList(cubes[:2]).merge_cube()\n",
    "print(merged.slices(['latitude', 'longitude']).next())\n",
    "\n",
    "print(cubes[0])\n",
    "# print(merged)\n",
    "# print(subd[0])\n",
    "# print(subd[1])\n",
    "# print(masked_merge(subd, ['time','realization', 'pressure']))\n",
    "print(masked_merge(subd, ['time','realization', 'pressure']))\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative_humidity / (%)             (time: 2; pressure: 3; latitude: 600; longitude: 800)\n",
      "     Dimension coordinates:\n",
      "          time                           x            -            -               -\n",
      "          pressure                       -            x            -               -\n",
      "          latitude                       -            -            x               -\n",
      "          longitude                      -            -            -               x\n",
      "     Auxiliary coordinates:\n",
      "          forecast_period                x            x            -               -\n",
      "     Scalar coordinates:\n",
      "          forecast_reference_time: 2016-12-03 00:00:00\n",
      "          realization: 0\n",
      "     Attributes:\n",
      "          STASH: m01s16i256\n",
      "          source: Data from Met Office Unified Model\n",
      "          um_version: 10.4\n"
     ]
    }
   ],
   "source": [
    "def make_masked_cube_for_point(template_cube, coord_names, point):\n",
    "        c = template_cube.copy()\n",
    "        constraints = {}\n",
    "        for name in coord_names:\n",
    "            constraints[name] = c.coord(name).points[0]\n",
    "        constraint = iris.Constraint(**constraints)\n",
    "        new_cube = c.extract(constraint)\n",
    "        for i, coord_name in enumerate(coord_names):\n",
    "            new_cube.coord(coord_name).points = (point[i],)\n",
    "        lazy_data = new_cube.lazy_data()\n",
    "        data = np.ma.masked_all(lazy_data.shape, dtype=lazy_data.dtype)\n",
    "        np.ma.set_fill_value(data, lazy_data.fill_value)\n",
    "        new_cube.data = data\n",
    "        return new_cube\n",
    "\n",
    "def make_missing_cubes(cube, coord_names, template_cube):\n",
    "    missing = get_missing_points(cube, coord_names)\n",
    "    print(missing)\n",
    "    cubes = []\n",
    "    for miss in missing:\n",
    "        cubes.append(make_masked_cube_for_point(template_cube, coord_names, miss))\n",
    "    return cubes\n",
    "\n",
    "\n",
    "def masked_merge(cube_list, ortha_coords):\n",
    "    simple_merged_cube = cube_list.merge_cube()\n",
    "    new_cubes = make_missing_cubes(simple_merged_cube, ortha_coords, cube_list[0])\n",
    "    all_cubes = cube_list + new_cubes\n",
    "    return all_cubes.merge_cube()\n",
    "    \n",
    "\n",
    "def flatten(cube_list):\n",
    "    flattened = []\n",
    "    reject = []\n",
    "    for cube in cube_list:\n",
    "        for flat_cube in cube.slices(['latitude', 'longitude']): # Why lat long? \n",
    "            flattened.append(flat_cube)\n",
    "    return iris.cube.CubeList(flattened)\n",
    "\n",
    "\n",
    "def get_missing_points(flat_cube_list, dims):\n",
    "    # go through all cubes to find all the points in the problem space dims\n",
    "    points = []\n",
    "    dim_point_sets = {dim:set() for dim in dims}\n",
    "    for cube in flat_cube_list:\n",
    "        point = []\n",
    "        for dim in dims:\n",
    "            dim_value = cube.coord(dim).points[0]\n",
    "            point.append(dim_value)\n",
    "            dim_point_sets[dim].add(dim_value)    \n",
    "        points.append(point)\n",
    "    \n",
    "    # Place a True value in the problem space if we have that point\n",
    "    # TODO: could use dic and rev dic to speed up the problem/real space conversion\n",
    "    space_map = {dim:list(dim_point_sets[dim]) for dim in dims}\n",
    "    def to_prob_space(point):\n",
    "        return tuple(space_map[dims[i]].index(point[i]) for i in range(len(dims)))\n",
    "    \n",
    "    def to_real_space(indexs):\n",
    "        return tuple(space_map[dims[i]][indexs[i]] for i in range(len(dims)))\n",
    "    \n",
    "    problem_space = np.zeros([len(space_map[dim]) for dim in dims], dtype=bool)\n",
    "            \n",
    "    for point in points:\n",
    "        problem_space[to_prob_space(point)] = True\n",
    "    \n",
    "    missing = []\n",
    "    # Find all the missing points in the problem space and convert back in to cube space\n",
    "    for missing_point_in_problem_space in zip(*np.where(problem_space == False)):\n",
    "        missing.append(to_real_space(missing_point_in_problem_space))\n",
    "    \n",
    "    return missing\n",
    "\n",
    "\n",
    "def masked_merge(cubes, dims):\n",
    "    cubes = flatten(cubes)\n",
    "    missing_cubes = []\n",
    "    for point in get_missing_points(cubes, dims):\n",
    "        missing_cubes.append(make_masked_cube_for_point(cubes[0], dims, point))\n",
    "    cubes += missing_cubes\n",
    "    return cubes.merge_cube()\n",
    "    \n",
    "\n",
    "subd = iris.cube.CubeList([cubes[0]] + [cubes[-6]]   + [cubes[-1]])\n",
    "print(super_merge(subd, ['time', 'pressure','realization']))\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sort_and_unique_cells(items):\n",
    "    sorted_items = sorted(items, key=attrgetter('val'))\n",
    "    return [sorted_items[0]] + [sorted_items[i + 1] for i in range(len(sorted_items) -1) if not sorted_items[i + 1] == sorted_items[i] ]\n",
    "\n",
    "def get_unique_dims(all_dims):\n",
    "    \"\"\"\n",
    "    Takes a list of dims and returns a dict:\n",
    "    \n",
    "    {dim:count}\n",
    "    \n",
    "    \"\"\"\n",
    "    all_dims = np.array(all_dims, dtype=object)\n",
    "    i = 0\n",
    "    dim_set = {}\n",
    "    while i < len(all_dims):\n",
    "        \n",
    "        dim = all_dims[i]\n",
    "        \n",
    "        if any(dim == other_dim for other_dim in dim_set.keys()): \n",
    "            i += 1 # If we've seen this dim before contiune\n",
    "            continue\n",
    "        else:\n",
    "            dim_set[dim] = 0 # Else add it to our set. Add it with count 0 as we will perform the count next.\n",
    "            \n",
    "        matches = (dim == all_dims)\n",
    "        \n",
    "        # TODO: Could we use np.not_where to extract the non matching and then work from there?\n",
    "        \n",
    "        dim_set[dim] = np.where(matches)[0].shape[0] # Count the occurances of this dimention. \n",
    "        # Looking forward in the list of dimentions are there any other dims that don't match\n",
    "        idx_of_forward_non_matches = np.where(matches[i:] == False)[0] \n",
    "        print('--',idx_of_forward_non_matches) # indexes of non matches with current dim offset by current posision. \n",
    "        \n",
    "        \n",
    "        if not len(idx_of_forward_non_matches):\n",
    "            break; # We've found all dims. Break.\n",
    "            \n",
    "        # If there are non-matches continue our search from the first location of a non match.\n",
    "        i = (idx_of_forward_non_matches[0] + i)\n",
    "    return dim_set\n",
    "    \n",
    "print(as_list_rep)\n",
    "\n",
    "(count_unique_dims(as_list)).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"class iris.coords.Coord(points, standard_name=None, long_name=None, var_name=None, units='1', bounds=None, attributes=None, coord_system=None)\n",
    "\"\"\"\n",
    "p0 = draw[0].coord('pressure')\n",
    "p1 = draw[1].coord('pressure')\n",
    "\n",
    "list(p0.cells())+ list(p1.cells())\n",
    "list(p0.cells())[0].bound\n",
    "\n",
    "\n",
    "dims = [p0, p1]\n",
    "\n",
    "def dim_list_to_single_dim(dim_list):\n",
    "    def to_points_and_bounds(list_of_dims):\n",
    "        points, bounds = list(zip(*[(item.point, item.bound) for sublist in [list(p.cells()) for p in list_of_dims] for item in sublist]))\n",
    "        return points, bounds\n",
    "\n",
    "    eg_dim = dim_list[0]\n",
    "    points, bounds = to_points_and_bounds(dim_list)\n",
    "    bounds = bounds if all(bounds) else None\n",
    "    new_dim = iris.coords.DimCoord(points=points, bounds=bounds,\n",
    "                         standard_name=eg_dim.standard_name, units=eg_dim.units,\n",
    "                         long_name=eg_dim.long_name, attributes=eg_dim.attributes,\n",
    "                         coord_system=eg_dim.coord_system, circular = eg_dim.circular,\n",
    "                        var_name = eg_dim.var_name)\n",
    "    return new_dim\n",
    "\n",
    "dim_list_to_single_dim(dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that `set` and `collections.Counter` both use `__hash__` and not `__equ__` for compairing items."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
